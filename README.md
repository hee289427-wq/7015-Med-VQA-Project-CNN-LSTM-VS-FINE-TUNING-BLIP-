# 7015-Med-VQA-Project-CNN-LSTM-VS-FINE-TUNING-BLIP-
# Medical Visual Question Answering (Med-VQA)

This repository contains the code for a course project on Medical Visual Question Answering using the VQA-RAD dataset.

## Project Structure
- `01_data_preprocessing.ipynb`: Dataset preprocessing and train/validation split.
- `02_cnn_lstm_baseline.ipynb`: CNN baseline and CNN+LSTM multimodal model.
- `03_blip_finetuning.ipynb`: Zero-shot and fine-tuned BLIP experiments.

## Dataset
The experiments are conducted on the VQA-RAD dataset. Due to licensing restrictions, the dataset is not included in this repository.

## Requirements
The code is implemented in Python and uses PyTorch and HuggingFace Transformers.  
All experiments were run on Google Colab.

## Notes
This repository is for academic and educational purposes only.
